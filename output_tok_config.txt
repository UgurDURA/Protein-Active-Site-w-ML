"{\"class_name\": \"Tokenizer\", \"config\": {\"num_words\": 0, \"filters\": \"#$%&()+/<=>?@\\\\^{|}~\", \"lower\": false, \"split\": \".\", \"char_level\": false, \"oov_token\": null, \"document_count\": 2, \"word_counts\": \"{\\\"4\\\": 1, \\\"1\\\": 3, \\\"15\\\": 1, \\\"5\\\": 1, \\\"99\\\": 1, \\\"-\\\": 1}\", \"word_docs\": \"{\\\"15\\\": 1, \\\"1\\\": 2, \\\"4\\\": 1, \\\"-\\\": 1, \\\"99\\\": 1, \\\"5\\\": 1}\", \"index_docs\": \"{\\\"3\\\": 1, \\\"1\\\": 2, \\\"2\\\": 1, \\\"6\\\": 1, \\\"5\\\": 1, \\\"4\\\": 1}\", \"index_word\": \"{\\\"1\\\": \\\"1\\\", \\\"2\\\": \\\"4\\\", \\\"3\\\": \\\"15\\\", \\\"4\\\": \\\"5\\\", \\\"5\\\": \\\"99\\\", \\\"6\\\": \\\"-\\\"}\", \"word_index\": \"{\\\"1\\\": 1, \\\"4\\\": 2, \\\"15\\\": 3, \\\"5\\\": 4, \\\"99\\\": 5, \\\"-\\\": 6}\"}}"